{
  "metadata": {
    "name": "Classification",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Overview\n\nThis notebook will show you how to create and query a table or DataFrame that you uploaded to DBFS. [DBFS](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html) is a Databricks File System that allows you to store data for querying inside of Databricks. This notebook assumes that you have a file already inside of DBFS that you would like to read from.\n\nThis notebook is written in **Python** so the default cell type is Python. However, you can use different languages by using the `%LANGUAGE` syntax. Python, Scala, SQL, and R are all supported."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### To Import Spark SQL and Spark ML Libraries \nTo Import Spark SQL and Spark ML Libraries. It is neccessary to access the functions."
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.types import *\n\nfrom pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit, CrossValidator\nfrom pyspark.sql import SparkSession\nfrom pyspark.context import SparkContext\n\nfrom pyspark.sql.functions import *\nfrom pyspark.ml.feature import *\nfrom pyspark.ml import *\nfrom pyspark.ml.classification import *\nfrom pyspark.ml.evaluation import *\nfrom pyspark.mllib.evaluation import *"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### To run the code in PySpark CLI\n\nSet the following to True:\n\nPYSPARK_CLI \u003d True"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nIS_DB \u003d True # Run the code in Databricks\n\nPYSPARK_CLI \u003d False\nif PYSPARK_CLI:\n    sc \u003d SparkContext.getOrCreate()\n    spark \u003d SparkSession(sc)"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\neCommerceSchema \u003d StructType([\n  StructField(\"event_time\", TimestampType(), False),\n  StructField(\"event_type\", StringType(), False),\n  StructField(\"product_id\", IntegerType(), False),\n  StructField(\"category_id\", LongType(), False),\n  StructField(\"StringType\", StringType(), False),\n  StructField(\"brand\", StringType(), False),\n  StructField(\"price\", DoubleType(), False),\n  StructField(\"user_id\", IntegerType(), False),\n  StructField(\"user_session\", StringType(), False),\n])"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Load Source Data\nThe data for this exercise is provided as a CSV file containing details of e-commerce items and catetegories. The data includes specific characteristics (or *features*) for each item, as well as a *label* column indicating what is the Event Type of each item.\n\nYou will load this data into a DataFrame and display it.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Read the csv file from HDFS (Hadoop File System)"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# File location and type\nfile_location \u003d \"/user/apandey9/5560/2019-Oct_Master.csv\"\nfile_type \u003d \"csv\"\n\n# CSV options\ninfer_schema \u003d \"true\"\nfirst_row_is_header \u003d \"true\"\ndelimiter \u003d \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf \u003d spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location) \n\ndf \u003d df.withColumnRenamed(\u0027# File format is event_time\u0027, \u0027event_time\u0027)\ndf \u003d df.filter((col(\"brand\") !\u003d \"No value\") \u0026 (col(\"category_code\") !\u003d \"No value\"))\n\ndf.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Converting the string type columns into integer using withColumn"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf \u003d df.withColumn(\"event_type\", when(df.event_type \u003d\u003d \u0027view\u0027, 1) \\\n                  .when(df.event_type \u003d\u003d \u0027cart\u0027, 2) \\\n                  .when(df.event_type \u003d\u003d \u0027purchase\u0027, 3))\ndf.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Converting the string type columns into indices using StringIndexer"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n#Converting String Values into Index Values\nindexer \u003d StringIndexer(inputCol\u003d\"brand\", outputCol\u003d\"brandIndex\")\nindexer1 \u003d StringIndexer(inputCol\u003d\"category_code\", outputCol\u003d\"category_codeIndex\")\ndf \u003d indexer.fit(df).transform(df) \ndf \u003d indexer1.fit(df).transform(df)\ndf.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf.printSchema()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Create a temporary view of the dataframe \"df\""
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Create a view or table\ntemp_table_name \u003d \"2019-Oct_Master.csv\"\ndf.createOrReplaceTempView(temp_table_name)"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nif PYSPARK_CLI:\n    csv \u003d spark.read.csv(\u00272019-Oct_Master.csv\u0027, inferSchema\u003dTrue, header\u003dTrue)\nelse:\n    csv \u003d spark.sql(\"SELECT * FROM 2019-Oct_Master.csv\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Selecting features\nIn this following step, we are selecting the features that are useful for Event_Type Prediction."
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndata \u003d csv.select(\"product_id\", \"brandIndex\", \"category_codeIndex\", \"price\", \"user_id\", col(\"event_type\").alias(\"label\"))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Split the Data\nIt is common practice when building supervised machine learning models to split the source data, using some of it to train the model and reserving some to test the trained model. In this exercise, you will use 70% of the data for training, and reserve 30% for testing."
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Split the data\nsplits \u003d data.randomSplit([0.7, 0.3])\ntrain \u003d splits[0]\ntest \u003d splits[1].withColumnRenamed(\"label\", \"trueLabel\")\nprint (\"Training Rows:\", train.count(), \" Testing Rows:\", test.count())"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\r\n## DECISION TREE CLASSIFIER"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\r\n### Prepare the Training Data\r\nTo train the regression model, you need a training data set that includes a vector of numeric features, and a label column. In this exercise, you will use the **VectorAssembler** class to transform the feature columns into a vector."
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nassembler_dt \u003d VectorAssembler(inputCols\u003d[\"product_id\", \"brandIndex\", \"category_codeIndex\", \"price\", \"user_id\" ], outputCol\u003d\"features\")\n\n#lr \u003d LinearRegression(labelCol\u003d\"label\", featuresCol\u003d\"normFeatures\")\ndt \u003d DecisionTreeClassifier(labelCol\u003d\"label\", featuresCol\u003d \"features\", maxBins\u003d3000)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\r\n### Parameter Building and Train using Cross Validator\r\nUsing the CrossValidator to evaluate each combination of parameters which are defined in ParameterGrid against multiple folds, in order to find the best performing parameters. It is used to find the best model for the data. Here the number of folds is assigned to 3."
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# TODO: params refered to the reference above\nparamGrid_cv_dt \u003d ParamGridBuilder() \\\n  .addGrid(dt.maxDepth, [3, 5]) \\\n  .addGrid(dt.minInfoGain, [0.0]) \\\n  .build()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Building the Pipeline\nDefine a pipeline that creates a feature vector and trains a Decision Tree model"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\npipeline_cv_dt \u003d Pipeline(stages\u003d[assembler_dt, dt])\n\nK \u003d 3\ncv_dt \u003d CrossValidator(estimator\u003dpipeline_cv_dt, evaluator\u003dMulticlassClassificationEvaluator(), estimatorParamMaps\u003dparamGrid_cv_dt, numFolds \u003d K)\n\nmodel_cv_dt \u003d cv_dt.fit(train)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\r\n### Test the Model\r\nNow you\u0027re ready to use the **transform** method of the model to generate some predictions. You can use this approach to predict Event Type where the label is unknown; but in this case you are using the test data which includes a known true label value, so you can compare the predicted event type to the actual event type."
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nprediction_cv_dt \u003d model_cv_dt.transform(test)\nprediction_cv_dt.select(\"features\", \"prediction\", \"trueLabel\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\r\n### Obtaning Accuracy, Test Error, Precision and Recall\r\nUsing the evaluation metrics as Accuracy, Test error, Precision and Recall. The Decision tree classifier model performance is calculated."
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nevaluator_cv_dt \u003d MulticlassClassificationEvaluator(labelCol\u003d\"trueLabel\", predictionCol\u003d\"prediction\", metricName\u003d\"accuracy\")\naccuracy_cv_dt \u003d evaluator_cv_dt.evaluate(prediction_cv_dt)\nprint (\"Average Accuracy \u003d\", accuracy_cv_dt)\nprint (\"Test Error \u003d \", (1 - accuracy_cv_dt))\n\nevaluator_cv_dt \u003d MulticlassClassificationEvaluator(labelCol\u003d\"trueLabel\", predictionCol\u003d\"prediction\", metricName\u003d\"weightedPrecision\")\nprecision_cv_dt \u003d evaluator_cv_dt.evaluate(prediction_cv_dt)\nprint (\"Precision \u003d\", precision_cv_dt)\n\nevaluator_cv_dt \u003d MulticlassClassificationEvaluator(labelCol\u003d\"trueLabel\", predictionCol\u003d\"prediction\", metricName\u003d\"weightedRecall\")\nRecall_cv_dt \u003d evaluator_cv_dt.evaluate(prediction_cv_dt)\nprint (\"Recall \u003d\", Recall_cv_dt)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "###References\n\nhttps://towardsdatascience.com/multi-class-text-classification-with-pyspark-7d78d022ed35\n\nhttps://spark.apache.org/docs/latest/ml-classification-regression.html#multilayer-perceptron-classifier\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ""
    }
  ]
}